= Лекция 1. Бэкроп, оптимизаторы, PyTorch

== Лектор(-ы)

Батраков Юрий
\@BatrakovYuri

Евдокимов Егор
\@ea_evdokimov

== План курса

Задумывался чисто про глубокое обучение. Но пришлось добавить ещё и нейронные сети.

=== Лекции и семинары

+ Бэкроп, оптимизаторы, PyTorch
+

=== Домашки и система оценивания

+ Базовые функции pytorch, классификация/регрессия, методы регуляризации *(15)*
+ Сегментация. Ускорение модели. *(10)*
+ NLP. Классификация текстов. Переводчик. NER. *(15)*
+ GAN + VAE. *(15)*
+ Representation learning. *(10)*
+ RecSys. ALS. *(10)*

\+ Экзамен *(40)*

Итог: Сумма баллов *(максимум 115)* делённая на 10. Округление математическое.



Сумма баллов за домашки

== Причём тут нейроны?

=== История

Нейроны -- штука, которая накапливает сигнал, при активации передаёт дальше. Нейронов очень много, (>80 млрд.)

Как это смоделировать?

Изначально хотели для использования машинного обучения, кодировать слова числами (просто прогнать словарь), но это оказалось неэффективно.

Давайте моделировать нейрон! Используем для этого:

=== Чуть-чуть матеши

$ (x_1, x_2, ..., x_n)$ -- векторное представление объекта, $n$ -- количество признаков.

$ (w_1, w_2, ..., w_n)$ -- коэффициенты 

$b$ -- смещение (bias) ("байас" по русски)

Легче представлять, что векторы $(1, x_1, ..., x_n)$ -- для удобства умножения $f$ -- какая-то функция (функция активации).

Какая функция? Например больше -- меньше. Если больше какай-то константы, то активируется (пороговая функция активации). 

Какая $f$ функция? Нелинейная, кусочно дифференцируемая.

$ a = f(sum_(j=1)^n w_(i,j) x_j + b) $

=== Какая аналогия с Нейронами?

// TODO: вставить картинку из презы.

=== Как это можно представить?

Теперь у нас $m$ выходов. Тогда выходы можно представить как матричное произведение матрицы весов на вектор.

$ W =
  mat(
    W_(0,1), ..., W_(0,m);
    ..., ..., ...;
    W_(d,1), ..., W_(d, m)
  ) in bb(R)^(d+1)
$

$x$

== Виды функций активаций

#figure(
  table(
    columns: (auto, auto, auto, auto, auto, auto),
    
    table.header([*ACTIVATION FUNCTION*], [*PLOT*], [*EQUATION*], [*DERIATIVE*], [*RANGE*], [*ЗАЧЕМ*]),

    [Линейная], [], [$ f(x)=x $], [ $ f(x)=1 $], [ $(-infinity, infinity)$ ], [],

    [Бинарный шаг],[],[],[],[],[Для классификации и удобства],

    [],[],[],[],[],[],

    [Гиперболический тангенс],[],[],[],[],[Для],

    [],[],[],[],[],[],

    [],[],[],[],[],[],

    [],[],[],[],[],[],

    [],[],[],[],[],[],
  )
)

// TODO:

=== Почему сети?

В сетях не один, а гораздо больше слоёв.

== Теория. Теорема Цыбенко.

Формальное изложение с википедии.

== Как оценить качество имеющегося решения?

Вводится несколько функций. Считаем ошибку. Вставить слайды из лекции.