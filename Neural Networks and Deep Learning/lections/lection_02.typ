= Лекция 2. [ТРЕБУЕТ ДОРАБОТКИ]

Два линейных слоя подряд это плохо, так как их можно объединить в один слой (умножения на 2 матрицы последовательно можно переписать в одну матрицу). Но иногда это может быть полезно, чтобы сузить размер промежуточный матрицы, чтобы она быстрее передавалась по информационному каналу. Бред, но не всегда автор -- дурак.

== Задача

Пришла Картинка, мы хотим понять. Кошечка это, или собачка?

Как мы это будем делать? 

После прохождение через нейронную сеть, мы получаем некоторый вектор, и засовываем его в SoftMax функцию активации. Она

$ (e^(z_i))/(sum_(j=1)^k z_j) $

== Что такое робастность?

Робастность модели -- устойчивость метода или модели к шуму, ошибкам и неожиданным ситуациям.

== Переобучение

После переобучения метрики могут улучшаться, но на практике такое не встречается.

== Взрыв и затухание градиента

Когда мы пропускаем градиенты через обратное распространение ошибки происходит перемножение большого количества чисел. Если эти числа будут большие, то градиент будет увеличиваться и когда back propagation дойдёт до последних слоёв, то мы собьём веса какие у нас были.

Когда градиенты на каждом слое маленькие, то после перемножения кучи маленьких чиссел совсем маленькие и последующие слои меняться совсем не будет 

== Инициализация весов

+ Инициализация константой (но веса будут меняться одинаково внутри слоёв = плохо)
+ Инициализация случайной величиной (плохо)
+ Xavier 
  Рассмотрим активацию: $y = w dot x + b = sum_i w_i x_i + b$
  Обозначим $w_i x_i = y_i$
  Тогда дисперсия этой величины
  $D(y_i) = ... = E[x_i]^2D(w_i) + E[w_i]^2 + D(x_i) + D(w_i)D(x_i)$

  Получили, что $D(y_i) = D(w_i)D(x_i)$

  (Data science $!=$ science)

  Сделаем предположение, что $x$ и $w$ сэмплируются независимо из одного распределение, (условно правда), тогда:

  $ D(y) = D(sum_(i=1)^*n) $ // TODO:

  Ранее веса инициализировали из распределения:

  $w_i $

  Xavier (backward Pass...) // TODO:


== Регулярцизация нейросетей

Тезники для борьбы с переобучением, повышением робастности и для получения более подходящего решения с точки зрения эксперта.

Изменяем:
+ Функции потери
+ Структуру сети
+ Процесс оптимизации
+ Данные

=== Меняем функцию потерь

Меняем функцию потерь, по которой считаем градиент:

$ "Loss"_"result" = "L"_"original" + L_"regularization" $

Добавка может быть любой, подходящей под задачу, но обычно используют

$L_1, L_2-"регуляризацию" "(weight decay)"$

Регуляризатор может быть применен как и к весам модели, так и к активациям / выходам, в зависимости от желаемых целей.

На практике ещё это требуется для добавления свойств, например, чтобы сделать вырожденным распределение (не по одной величине, а по нескольким) для целей компании.

$L_2-"регуляризация"$

// TODO: 

L_1 прореживает веса

L_2 также сжимает, но сильнее

/// TODO: $L_1$ от $L_2$$ отличается тем, что...
=== Меняем архитектуру

Меняем:
+ Специальные слои (добавляем!) (нормализации и DropOut)
+ Дистилляция
+ Квантизация
+ Пруннинг 

== нормализации

//TODO: